# -*- coding: utf-8 -*-
"""Feature Selection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v39-muYXzHTo7JiVIsh4X2fKKfHYqAIP
"""

import pandas as pd 
import numpy as np

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn import datasets, linear_model
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, precision_recall_curve, auc
from sklearn.feature_selection import f_classif
from sklearn.pipeline import Pipeline
from sklearn.base import BaseEstimator, TransformerMixin
from scipy.stats import chi2_contingency

"""# Loading the dataset"""

data = pd.read_excel(r"C:\Users\karteek\PycharmProjects\Covid_WNS\COVID_dataset.xlsx")

"""# get a list of columns that have more than 80% null values"""

na_values = data.isnull().mean()
na_values[na_values>0.8]

data.columns

data.info()

data.head()

y = data['Target']
X = data.drop(columns=['Target'])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)

X_train.columns

print (X_train.shape, y_train.shape)
print (X_test.shape, y_test.shape)

"""# first divide training data into categorical and numerical subsets"""

X_train_cat = X_train.select_dtypes(include = 'object').copy()
X_train_num = X_train.select_dtypes(include = 'number').copy()

X_train_num.columns

X_test_cat = X_test.select_dtypes(include = 'object').copy()
X_test_num = X_test.select_dtypes(include = 'number').copy()

X_train_num

"""# Constant Features Removal"""

from sklearn.feature_selection import VarianceThreshold
constant_filter = VarianceThreshold(threshold=0)
constant_filter.fit(X_train_num)
print(constant_filter.get_support().sum())
constant_list = [not temp for temp in constant_filter.get_support()]
print(constant_list)
#print(X.columns[constant_list])

X_train_num.columns[constant_list]

"""##### there is no constants in the data set

# Quasi constant feature removal
"""

quasi_constant_filter = VarianceThreshold(threshold=0.01)
quasi_constant_filter.fit(X_train_num)
VarianceThreshold(threshold=0.01)
quasi_constant_filter.get_support().sum()

quasi_constant_list = [not temp for temp in quasi_constant_filter.get_support()]
print(quasi_constant_list)

X_train_num.columns[quasi_constant_list]

"""####### one quasi constant is there"""

# To remove those quasi constant features, we need to apply transform on quasi transform filter object
X_train_quasi_filter = quasi_constant_filter.transform(X_train_num)
X_test_quasi_filter = quasi_constant_filter.transform(X_test_num)
X_train_quasi_filter.shape, X_test_quasi_filter.shape

"""# Remove Duplicate Features"""

X_train_T = X_train_quasi_filter.T
X_test_T = X_test_quasi_filter.T
type(X_train_T)
np.ndarray

X_train_T = pd.DataFrame(X_train_T)
X_test_T = pd.DataFrame(X_test_T)

X_train_T.duplicated().sum()

duplicated_features = X_train_T.duplicated()

features_to_keep = [not index for index in duplicated_features]
X_train_unique = X_train_T[features_to_keep].T
X_test_unique = X_test_T[features_to_keep].T

"""#### 0 duplicate features, so no need to remove any.

# Feature Selection with Filtering Method- Correlated Feature Removal
"""

corrmat = X_train_unique.corr()
plt.figure(figsize=(10,10))
sns.heatmap(corrmat, annot=True)

def get_correlation(data, threshold):
    corr_col = set()
    corrmat = data.corr()
    for i in range(len(corrmat.columns)):
        for j in range(i):
            if abs(corrmat.iloc[i, j])> threshold:
                colname = corrmat.columns[i]
                corr_col.add(colname)
    return corr_col

corr_features = get_correlation(X_train_unique, 0.85)
corr_features

X_train_uncorr = X_train_unique.drop(labels=corr_features, axis = 1)
X_test_uncorr = X_test_unique.drop(labels = corr_features, axis = 1)
X_train_uncorr.shape, X_test_uncorr.shape

X_train_uncorr

"""# Feature Selection Based on Univariate (ANOVA) Test for Classification"""

from sklearn.feature_selection import f_classif
from sklearn.feature_selection import SelectKBest, SelectPercentile

X_test_unique.fillna(X_test_unique.mean(), inplace = True)

# Now do F-Test
# since f_class_if does not accept missing values, we will do a very crude imputation of missing values
X_train_unique.fillna(X_train_unique.mean(), inplace = True)
# Calculate F Statistic and corresponding p values
sel = f_classif(X_train_unique, y_train)

p_values = pd.Series(sel[1])
p_values.index = X_train_unique.columns
p_values.sort_values(ascending = True, inplace = True)

p_values.plot.bar(figsize = (16, 5))
plt.title('pvalues with respect to features')
plt.show()

p_values = p_values[p_values<0.05]
p_values.index

X_train_p = X_train_unique[p_values.index]
X_test_p = X_test_unique[p_values.index]

X_train_p

# since f_class_if does not accept missing values, we will do a very crude imputation of missing values
X_train_num.fillna(X_train_num.mean(), inplace = True)
# Calculate F Statistic and corresponding p values
F_statistic, p_values = f_classif(X_train_num, y_train)
ANOVA_F_table = pd.DataFrame(data = {'Numerical_Feature': X_train_num.columns.values, 'F-Score': F_statistic, 'p values': p_values.round(decimals=10)})
ANOVA_F_table.sort_values(by = ['F-Score'], ascending = False, ignore_index = True, inplace = True)
ANOVA_F_table

"""# Feature Selection Based on Univariate ROC_AUC for Classification"""

from sklearn.ensemble import RandomForestClassifier
roc_auc = []
for feature in X_train_unique.columns:
    clf = RandomForestClassifier(n_estimators=100, random_state=0)
    clf.fit(X_train_unique[feature].to_frame(), y_train)
    y_pred = clf.predict(X_test_unique[feature].to_frame())
    roc_auc.append(roc_auc_score(y_test, y_pred))

print(roc_auc)

roc_values = pd.Series(roc_auc)
roc_values.index = X_train_unique.columns
roc_values.sort_values(ascending =False, inplace = True)

roc_values.plot.bar()
plt.title('roc_auc score with respect to the features')
plt.show()

sel = roc_values[roc_values>0.5]
sel

X_train_roc = X_train_unique[sel.index]
X_test_roc = X_test_unique[sel.index]
X_train_roc.shape

"""# Mutual Information (Entropy) Gain for Classification """

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.feature_selection import VarianceThreshold, mutual_info_classif, mutual_info_regression
from sklearn.feature_selection import SelectKBest, SelectPercentile

mi = mutual_info_classif(X_train_unique, y_train) 
len(mi)

mi = pd.Series(mi)
mi.index = X_train_unique.columns

mi.sort_values(ascending=False, inplace = True)

plt.title('Mutual information with respect to features')
mi.plot.bar(figsize = (16,5))
plt.show()

sel = SelectPercentile(mutual_info_classif, percentile=70).fit(X_train_unique, y_train)
X_train_unique.columns[sel.get_support()]

X_train_mi = sel.transform(X_train_unique)
X_test_mi = sel.transform(X_test_unique)
X_train_mi.shape

"""# Step Forward, Step Backward and Exhaustive Feature Selection"""

!pip install mlxtend

from mlxtend.feature_selection import SequentialFeatureSelector as SFS
sfs = SFS(RandomForestClassifier(n_estimators=100, random_state=0, n_jobs = -1),
         k_features = 23,
          forward= True,
          floating = False,
          verbose= 2,
          scoring= 'accuracy',
          cv = 4,
          n_jobs= -1
         ).fit(X_train_num, y_train)

sfs.k_feature_names_

import pandas as pd
sfs_data = pd.DataFrame.from_dict(sfs.get_metric_dict()).T

from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs
fig1 = plot_sfs(sfs.get_metric_dict(), kind='std_dev')

plt.ylim([0.8, 1])
plt.title('Sequential Forward Selection (w. StdDev)')
plt.grid()
plt.show()

"""# Step Backward Selection (SBS)"""

sbs = SFS(RandomForestClassifier(n_estimators=100, random_state=0, n_jobs = -1),
         k_features = 25,
          forward= False,
          floating = False,
          verbose= 2,
          scoring= 'accuracy',
          cv = 4,
          n_jobs= -1
         ).fit(X_train_num, y_train)

print(sbs.k_score_)
sbs.k_feature_names_

import pandas as pd
pd.DataFrame.from_dict(sbs.get_metric_dict()).T

from mlxtend.plotting import plot_sequential_feature_selection as plot_sbs
fig1 = plot_sbs(sbs.get_metric_dict(), kind='std_dev')

plt.ylim([0.8, 1])
plt.title('Sequential Forward Selection (w. StdDev)')
plt.grid()
plt.show()

"""# Recursive Feature Elimination (RFE) by Using Tree Based and Gradient Based Estimators """

from yellowbrick.model_selection import RFECV

visualizer = RFECV(LogisticRegression())
visualizer.fit(X_train_num, y_train)        # Fit the data to the visualizer
visualizer.show()









from sklearn.feature_selection import SelectFromModel
from sklearn.feature_selection import RFE
sel = RFE(RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1), n_features_to_select = 25)
sel.fit(X_train_unique, y_train)

sel.get_support()

features = X_train_unique.columns[sel.get_support()]
features

X_train_rfe = sel.transform(X_train_unique)
X_test_rfe = sel.transform(X_test_unique)

"""# Permutation Based Feature Importance """

rf = RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1)
rf.fit(X_train_unique,y_train)

rf.feature_importances_

from sklearn.inspection import permutation_importance
perm_importance = permutation_importance(rf, X_test_unique, y_test)
sorted_idx = perm_importance.importances_mean.argsort()
plt.barh(X_train_unique.columns[sorted_idx], perm_importance.importances_mean[sorted_idx])
plt.xlabel("Permutation Importance")

sorted_idx

"""# Use of Linear and Logistic Regression Coefficients with Lasso (L1) and Ridge (L2) Regularization """

from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.feature_selection import SelectFromModel
sel = SelectFromModel(LinearRegression())
sel.fit(X_train_unique, y_train)

sel.get_support()

sel.estimator_.coef_

mean = np.mean(np.abs(sel.estimator_.coef_))
mean

np.abs(sel.estimator_.coef_)

X_train_reg = sel.transform(X_train_unique)
X_test_reg = sel.transform(X_test_unique)
X_test_reg.shape

"""# Logistic Regression Coefficient with L1 Regularization"""

sel = SelectFromModel(LogisticRegression(penalty = 'l1', C = 0.05, solver = 'liblinear'))
sel.fit(X_train_unique, y_train)
sel.get_support()

sel.estimator_.coef_

X_train_l1 = sel.transform(X_train_unique)
X_test_l1 = sel.transform(X_test_unique)

def run_randomForest(X_train, X_test, y_train, y_test):
    clf = RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1)
    clf = clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    print('AUC_ROC: ', roc_auc_score(y_test, y_pred))

# Commented out IPython magic to ensure Python compatibility.
# %%time
# run_randomForest(X_train_l1, X_test_l1, y_train, y_test)

"""# L2 Regularization"""

sel2 = SelectFromModel(LogisticRegression(penalty = 'l2', C = 0.05, solver = 'liblinear'))
sel2.fit(X_train_unique, y_train)
sel2.get_support()

sel2.estimator_.coef_

X_train_l2 = sel2.transform(X_train_unique)
X_test_l2 = sel2.transform(X_test_unique)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# run_randomForest(X_train_l1, X_test_l1, y_train, y_test)







